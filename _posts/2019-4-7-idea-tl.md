---
layout:     post
title:      "Search_可解释性调研与迁移学习"
subtitle:   " \"经过思路的整理，对用什么方法进行实验做了调研\""
date:       2019-04-7 13:00:00
author:     "jack"
header-img: "img/post-bg-forest.jpg"
catalog: true
tags:
    - 可解释性
    - 迁移学习
    - 深度学习
---

## 可解释性研究调研与迁移学习

### 1.理解迁移学习

在我们确定要去度量差距之后，我的重点就放在了度量方法的研究上。通过了解迁移学习的概念，我认为迁移学习大致就是在做**源域到目标域的迁移**。通过度量两者之间的差距来调整或者`finetune`模型(上节组会还不知道这个名词，没想到在这遇见了)，而且我认为这些差距的度量其实就是在搞**分布概率和数值差距**来得到一个最优化目标（有点像多目标优化问题）。经典迁移算法`JDA`就是一个多优化问题

重新理解我们的问题，我们目标也就是两个：在输出的feature map上度量时

1. 同类之间的差距小，不同类之间的差距大
2. 概率（准确率）高

其实也就是**分布概率和数值差距**，所以跟迁移学习有点相似。而且我们更关注**数值差距问题**。概率是结果

### 2. 度量方法

#### 2.1 基本距离方法

1. 欧式距离，衡量两个向量（空间上的点的距离）

   由于feature map数值上太不靠谱，我们要用的话，可以进行GAP之后压缩的1维的**标准化**欧式距离。

   ![](https://ws1.sinaimg.cn/large/007bgNxTly1g1t2j2d6yhj317i05omx6.jpg)

2. 曼哈顿距离，直觉是基本上没法用

3. 马氏距离，有点像标准欧式距离，先将两个点投射到一个分布中，再加协方差，考虑方差

   ![](https://ws1.sinaimg.cn/large/007bgNxTly1g1t2prbriqj315x03nq2w.jpg)

   最典型的就是根据距离作判别问题，即假设有n个总体，计算某个样品X归属于哪一类的问题。此时虽然样品X离某个总体的欧氏距离最近，但是未必归属它，比如该总体的方差很小，说明需要非常近才能归为该类。对于这种情况，马氏距离比欧氏距离更适合作判别。

4. 余弦相似度 

​        衡量两个向量的相关性 (夹角的余弦)。

5. 互信息 

​        定义在两个概率分布 X,Y 上，`x ∈ X,y ∈ Y `

#### 2.2 流形学习

我们关注重点在数据上，那么这个feature map的多维数据怎么看出它的差距就是关键

**Grassmann** 流形 G(d) 可以通过将原始的 d 维子空间 (特征向量) 看作它基础的元素，从而可以帮助学习分类器。在 **Grassmann** 流形中，特征变换和分布适配通常都有着有效的数值形式，因此在迁移学习问题中可以被很高效地表示和求解 。因此，我感觉利用 **Grassmann** 流形空间中来进行处理也是可行的。

#### 2.3 MMD 

目前觉得最好的

maximum mean discrepancy，最先提出的时候用于双样本的检测（two-sample test）问题，用于判断两个分布p和q是否相同。它的基本假设是：如果对于所有以分布生成的样本空间为输入的函数f，如果两个分布生成的足够多的样本在f上的对应的像的均值都相等，那么那么可以认为这两个分布是同一个分布。现在一般用于度量两个分布之间的相似性

基于两个分布的样本，通过寻找在样本空间上的连续函数f，求不同分布的样本在f上的函数值的均值，通过把两个均值作差可以得到两个分布对应于f的mean discrepancy。寻找一个f使得这个mean discrepancy有最大值，就得到了MMD。最后取MMD作为检验统计量（test statistic），从而判断两个分布是否相同。如果这个值足够小，就认为两个分布相同，否则就认为它们不相同。同时这个值也用来判断两个分布之间的相似程度。如果用F表示一个在样本空间上的连续函数集，那么MMD可以用下面的式子表示： 

### 3. 深度学习与迁移学习

**先看这个图吧，真的对我的建议很大**，这是在两个不同的训练集上面进行迁移学习。而且使用loss的方法对两个网络fc的output array 进行惩罚来学习，不就是我们的第二种3.2事中学习的思想吗？对两个数据来进行比较。他们是两类，我们是一组（多个）。

#### 3.1 DCC

这个方法是加州大学伯克利分校的 Tzeng 等人提出的， DDC 方法 (Deep Domain Confusion) 解决深度网络的自适应问题。DDC 固定了 AlexNet 的前 7 层，在第 8 层 (分类器前一 层) 上加入了自适应的度量。自适应度量方法采用了被广泛使用的 MMD 准则。DDC 方法 的损失函数表示为：

![](https://ws1.sinaimg.cn/large/007bgNxTly1g1t47ixpllj30ew0fwtap.jpg)



#### 3.2 DAN

来自清华大学的龙明盛等人在 2015 年发表在机器学习顶级会议 ICML 上的 DAN 方法 (Deep Adaptation Networks) 对 DDC 方法进行了几个方面的扩展。

- 有别于 DDC 方法只加入一个自适应层，DAN 方法同时加入了三个自适应层 (分类器前三层)。
- DAN方法采用了表征能力更好的多核MMD度量(MK-MMD代替了 DDC 方法中的单一核 MMD。
- DAN 方法将多核 MMD 的参数学习融入到深度网络的训练中，不增加网络的额外训练时间。DAN 方法在多个任务上都取得了比 DDC 更好的分类效果。 

![](https://ws1.sinaimg.cn/large/007bgNxTly1g1t49eytfuj30j00f5jvt.jpg)

也使用了t-she降维和对比了DDC和DAN的差距

![](https://ws1.sinaimg.cn/large/007bgNxTly1g1t4ajafa1j30z60ajdjh.jpg)

大致看了看这两篇论文，后面找时间再拜读。

[Learning Transferable Features with Deep Adaptation Networks](https://arxiv.org/pdf/1502.02791)

[DeepDomainConfusion: MaximizingforDomainInvariance](https://arxiv.org/pdf/1412.3474.pdf)

### 4. 反思

- 现在最大的想法就是这个DDC和DAN应该都可以用，因为基本思想一致嘛，但是他们用于度量差距的不管是MMD还是多核的MMD都是训练得到的，而我的想法想要的就是一个评价标准
- 针对上一点，我现在的大致想法是，可以对输入的MMD训练，输出的验证。